{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLyGGNWqob-I"
      },
      "source": [
        "mounting to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14DYKVj3CBiT",
        "outputId": "5c0737cd-a5a0-4314-9b86-61e524a6e2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkl_cXdPogPo"
      },
      "source": [
        "install library prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrL3tlE5ECcu",
        "outputId": "8205282c-dcba-49b0-e7da-c780777b5313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scikit-learn pillow tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zydXBpseyKVs"
      },
      "source": [
        "script for finding the folders in drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xLf-xgi9rm5K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the contents of the ASL_to_Text_Project directory\n",
        "project_dir = '/content/drive/MyDrive/ASL_to_Text_Project'\n",
        "print(f\"\\nContents of {project_dir}:\")\n",
        "print(os.listdir(project_dir))\n",
        "\n",
        "# Print the contents of the data directory\n",
        "data_dir = os.path.join(project_dir, 'data')\n",
        "print(f\"\\nContents of {data_dir}:\")\n",
        "print(os.listdir(data_dir))"
      ],
      "metadata": {
        "id": "KCD4vP7dZ_jK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e7cd29-8003-401d-d764-a0f18464b5f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contents of /content/drive/MyDrive/ASL_to_Text_Project:\n",
            "['data', 'models']\n",
            "\n",
            "Contents of /content/drive/MyDrive/ASL_to_Text_Project/data:\n",
            "['labels', 'images']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOKpNOQ5oZo1"
      },
      "source": [
        "training script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "trEyuxtYElJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c102d243-b713-4a86-e10a-0f59f86809e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution:\n",
            "G: 500\n",
            "F: 500\n",
            "C: 500\n",
            "H: 500\n",
            "E: 500\n",
            "I love you: 500\n",
            "B: 500\n",
            "A: 500\n",
            "I: 500\n",
            "D: 500\n",
            "Q: 513\n",
            "L: 500\n",
            "M: 500\n",
            "P: 500\n",
            "N: 500\n",
            "No: 500\n",
            "Please: 500\n",
            "O: 500\n",
            "J: 500\n",
            "K: 500\n",
            "S: 500\n",
            "V: 500\n",
            "Thank you: 500\n",
            "Unknown: 500\n",
            "X: 500\n",
            "U: 500\n",
            "T: 500\n",
            "W: 500\n",
            "R: 500\n",
            "Y: 500\n",
            "Yes: 500\n",
            "Z: 500\n",
            "Loaded 16013 images with shape (16013, 227, 227, 3)\n",
            "Unique labels: ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'I love you' 'J' 'K' 'L' 'M' 'N' 'No'\n",
            " 'O' 'P' 'Please' 'Q' 'R' 'S' 'T' 'Thank you' 'U' 'Unknown' 'V' 'W' 'X'\n",
            " 'Y' 'Yes' 'Z']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 336ms/step - accuracy: 0.2209 - loss: 3.4634 - precision: 0.3529 - recall: 0.1528 - val_accuracy: 0.0312 - val_loss: 10.0946 - val_precision: 0.0327 - val_recall: 0.0312 - learning_rate: 1.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m  1/400\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7188 - loss: 1.0042 - precision: 0.7333 - recall: 0.6875"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7188 - loss: 1.0042 - precision: 0.7333 - recall: 0.6875 - val_accuracy: 0.0312 - val_loss: 10.0562 - val_precision: 0.0327 - val_recall: 0.0312 - learning_rate: 1.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 313ms/step - accuracy: 0.7187 - loss: 0.8767 - precision: 0.7606 - recall: 0.6780 - val_accuracy: 0.9307 - val_loss: 0.1885 - val_precision: 0.9340 - val_recall: 0.9279 - learning_rate: 1.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.8314 - precision: 0.9000 - recall: 0.8438 - val_accuracy: 0.9310 - val_loss: 0.1894 - val_precision: 0.9341 - val_recall: 0.9288 - learning_rate: 1.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 312ms/step - accuracy: 0.8883 - loss: 0.3488 - precision: 0.9002 - recall: 0.8772 - val_accuracy: 0.9241 - val_loss: 0.2513 - val_precision: 0.9247 - val_recall: 0.9235 - learning_rate: 1.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.2472 - precision: 0.9375 - recall: 0.9375 - val_accuracy: 0.9017 - val_loss: 0.3127 - val_precision: 0.9041 - val_recall: 0.9004 - learning_rate: 1.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 312ms/step - accuracy: 0.9325 - loss: 0.2070 - precision: 0.9375 - recall: 0.9284 - val_accuracy: 0.9953 - val_loss: 0.0100 - val_precision: 0.9953 - val_recall: 0.9953 - learning_rate: 1.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.1131 - precision: 0.9688 - recall: 0.9688 - val_accuracy: 0.9944 - val_loss: 0.0123 - val_precision: 0.9944 - val_recall: 0.9944 - learning_rate: 1.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 312ms/step - accuracy: 0.9625 - loss: 0.1246 - precision: 0.9660 - recall: 0.9595 - val_accuracy: 0.9903 - val_loss: 0.0328 - val_precision: 0.9906 - val_recall: 0.9900 - learning_rate: 1.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.2851 - precision: 0.9375 - recall: 0.9375 - val_accuracy: 0.9906 - val_loss: 0.0326 - val_precision: 0.9906 - val_recall: 0.9906 - learning_rate: 1.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 312ms/step - accuracy: 0.9725 - loss: 0.0856 - precision: 0.9741 - recall: 0.9706 - val_accuracy: 0.9835 - val_loss: 0.0426 - val_precision: 0.9841 - val_recall: 0.9825 - learning_rate: 1.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.0839 - precision: 0.9688 - recall: 0.9688 - val_accuracy: 0.9813 - val_loss: 0.0482 - val_precision: 0.9816 - val_recall: 0.9803 - learning_rate: 1.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 312ms/step - accuracy: 0.9704 - loss: 0.0870 - precision: 0.9724 - recall: 0.9694 - val_accuracy: 0.9791 - val_loss: 0.0716 - val_precision: 0.9791 - val_recall: 0.9791 - learning_rate: 1.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.0364 - precision: 0.9688 - recall: 0.9688 - val_accuracy: 0.9816 - val_loss: 0.0675 - val_precision: 0.9816 - val_recall: 0.9816 - learning_rate: 1.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 315ms/step - accuracy: 0.9866 - loss: 0.0465 - precision: 0.9871 - recall: 0.9858 - val_accuracy: 1.0000 - val_loss: 9.5077e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 16/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.0432 - precision: 0.9688 - recall: 0.9688 - val_accuracy: 1.0000 - val_loss: 9.0849e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 17/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 314ms/step - accuracy: 0.9928 - loss: 0.0194 - precision: 0.9930 - recall: 0.9927 - val_accuracy: 1.0000 - val_loss: 2.6468e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 18/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.9902e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 2.6615e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 19/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 311ms/step - accuracy: 0.9943 - loss: 0.0209 - precision: 0.9943 - recall: 0.9940 - val_accuracy: 0.9984 - val_loss: 0.0044 - val_precision: 0.9984 - val_recall: 0.9984 - learning_rate: 2.0000e-05\n",
            "Epoch 20/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0129 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9984 - val_loss: 0.0044 - val_precision: 0.9984 - val_recall: 0.9984 - learning_rate: 2.0000e-05\n",
            "Epoch 21/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 311ms/step - accuracy: 0.9945 - loss: 0.0165 - precision: 0.9952 - recall: 0.9944 - val_accuracy: 1.0000 - val_loss: 2.3817e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 22/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.9511e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 2.6745e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 23/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 312ms/step - accuracy: 0.9962 - loss: 0.0118 - precision: 0.9963 - recall: 0.9962 - val_accuracy: 1.0000 - val_loss: 1.3284e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 4.0000e-06\n",
            "Epoch 24/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0017 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 1.3186e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 4.0000e-06\n",
            "Epoch 25/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 312ms/step - accuracy: 0.9960 - loss: 0.0106 - precision: 0.9963 - recall: 0.9956 - val_accuracy: 0.9997 - val_loss: 4.5793e-04 - val_precision: 0.9997 - val_recall: 0.9997 - learning_rate: 4.0000e-06\n",
            "Epoch 26/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.4869e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9997 - val_loss: 4.4949e-04 - val_precision: 0.9997 - val_recall: 0.9997 - learning_rate: 4.0000e-06\n",
            "Epoch 27/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 312ms/step - accuracy: 0.9967 - loss: 0.0115 - precision: 0.9967 - recall: 0.9967 - val_accuracy: 1.0000 - val_loss: 1.9205e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 4.0000e-06\n",
            "Epoch 28/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.3581e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_loss: 2.0194e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 4.0000e-06\n",
            "Epoch 29/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 313ms/step - accuracy: 0.9965 - loss: 0.0101 - precision: 0.9966 - recall: 0.9963 - val_accuracy: 0.9994 - val_loss: 7.2824e-04 - val_precision: 0.9994 - val_recall: 0.9994 - learning_rate: 4.0000e-06\n",
            "Epoch 30/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0080 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9994 - val_loss: 7.4916e-04 - val_precision: 0.9994 - val_recall: 0.9994 - learning_rate: 8.0000e-07\n",
            "Epoch 31/100\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 312ms/step - accuracy: 0.9977 - loss: 0.0079 - precision: 0.9977 - recall: 0.9976 - val_accuracy: 0.9994 - val_loss: 7.1778e-04 - val_precision: 0.9994 - val_recall: 0.9994 - learning_rate: 8.0000e-07\n",
            "101/101 - 1s - 7ms/step - accuracy: 1.0000 - loss: 9.0849e-05 - precision: 1.0000 - recall: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test accuracy: 1.0\n",
            "Test precision: 1.0\n",
            "Test recall: 1.0\n",
            "Model saved successfully: /content/drive/MyDrive/ASL_to_Text_Project/models/improved_balanced_asl_model.h5\n",
            "Label Encoder saved successfully: /content/drive/MyDrive/ASL_to_Text_Project/data/labels/label_encoder.pkl\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE = 227\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "DATA_DIR = \"/content/drive/MyDrive/ASL_to_Text_Project/data\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/ASL_to_Text_Project/models\"\n",
        "LABELS_DIR = \"/content/drive/MyDrive/ASL_to_Text_Project/data/labels\"\n",
        "\n",
        "\n",
        "# Configuration\n",
        "\n",
        "def create_improved_model(num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(64, 11, strides=4, padding=\"same\", activation=\"relu\", input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(3, strides=2),\n",
        "        Conv2D(192, 5, padding=\"same\", activation=\"relu\"),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(3, strides=2),\n",
        "        Conv2D(384, 3, padding=\"same\", activation=\"relu\"),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(256, 3, padding=\"same\", activation=\"relu\"),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(256, 3, padding=\"same\", activation=\"relu\"),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(3, strides=2),\n",
        "        Flatten(),\n",
        "        Dense(4096, activation=\"relu\"),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(4096, activation=\"relu\"),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def load_balanced_data(data_dir, img_size=IMG_SIZE, max_samples_per_class=1000):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_counts = {}\n",
        "    images_dir = os.path.join(data_dir, \"images\")\n",
        "\n",
        "    for label in os.listdir(images_dir):\n",
        "        label_dir = os.path.join(images_dir, label)\n",
        "        if os.path.isdir(label_dir):\n",
        "            class_counts[label] = 0\n",
        "            for img_name in os.listdir(label_dir):\n",
        "                if class_counts[label] >= max_samples_per_class:\n",
        "                    break\n",
        "                img_path = os.path.join(label_dir, img_name)\n",
        "                if os.path.isfile(img_path):\n",
        "                    try:\n",
        "                        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_size, img_size))\n",
        "                        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "                        images.append(img_array)\n",
        "                        labels.append(label)\n",
        "                        class_counts[label] += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    print(\"Class distribution:\")\n",
        "    for label, count in class_counts.items():\n",
        "        print(f\"{label}: {count}\")\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def main():\n",
        "    X, y = load_balanced_data(DATA_DIR)\n",
        "\n",
        "    if len(X) == 0:\n",
        "        print(\"No images were loaded. Please check the data directory structure.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loaded {len(X)} images with shape {X.shape}\")\n",
        "    print(f\"Unique labels: {np.unique(y)}\")\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "    num_classes = len(le.classes_)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "\n",
        "    X_train = X_train / 255.0\n",
        "    X_test = X_test / 255.0\n",
        "\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    train_generator = train_datagen.flow(\n",
        "        X_train, y_train, batch_size=BATCH_SIZE, shuffle=True\n",
        "    )\n",
        "\n",
        "    model = create_improved_model(num_classes)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
        "    )\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=7)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=(X_test, y_test),\n",
        "        callbacks=[early_stopping, lr_scheduler]\n",
        "    )\n",
        "\n",
        "    test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=2)\n",
        "    print(f\"\\nTest accuracy: {test_acc}\")\n",
        "    print(f\"Test precision: {test_precision}\")\n",
        "    print(f\"Test recall: {test_recall}\")\n",
        "\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "    model_path = os.path.join(MODEL_DIR, \"improved_balanced_asl_model.h5\")\n",
        "    model.save(model_path)\n",
        "    print(f\"Model saved successfully: {model_path}\")\n",
        "\n",
        "    os.makedirs(LABELS_DIR, exist_ok=True)\n",
        "    label_encoder_path = os.path.join(LABELS_DIR, \"label_encoder.pkl\")\n",
        "    with open(label_encoder_path, \"wb\") as f:\n",
        "        pickle.dump(le, f)\n",
        "    print(f\"Label Encoder saved successfully: {label_encoder_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}