{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLyGGNWqob-I"
      },
      "source": [
        "# mounting to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14DYKVj3CBiT",
        "outputId": "f72d0b66-0bc0-491a-aa62-350581929096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkl_cXdPogPo"
      },
      "source": [
        "# install library prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrL3tlE5ECcu",
        "outputId": "73ef8afd-c878-45fe-ea82-401a7f0971e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scikit-learn pillow tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "cgvhmT_WQ-8j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLf-xgi9rm5K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zydXBpseyKVs"
      },
      "source": [
        "# script for finding the folders in drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the contents of the ASL_to_Text_Project directory\n",
        "project_dir = '/content/drive/MyDrive/ASL_to_Text_Project'\n",
        "print(f\"\\nContents of {project_dir}:\")\n",
        "print(os.listdir(project_dir))\n",
        "\n",
        "# Print the contents of the data directory\n",
        "data_dir = os.path.join(project_dir, 'data')\n",
        "print(f\"\\nContents of {data_dir}:\")\n",
        "print(os.listdir(data_dir))"
      ],
      "metadata": {
        "id": "KCD4vP7dZ_jK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0e7ef0-9f40-4e79-ee34-f8dc098df5d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contents of /content/drive/MyDrive/ASL_to_Text_Project:\n",
            "['data', 'models']\n",
            "\n",
            "Contents of /content/drive/MyDrive/ASL_to_Text_Project/data:\n",
            "['labels', 'images']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utilizing alexnet code"
      ],
      "metadata": {
        "id": "LPZ2VL4Jl0_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- Configuration ---------------------\n",
        "IMG_SIZE = 227  # AlexNet input size\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "DATA_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/data\" # Changed to match the correct path in Google Drive\n",
        "IMAGES_DIR = os.path.join(DATA_DIR, 'images')  # Path to the 'images' folder\n",
        "MODEL_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/models\" # Changed to match the correct path in Google Drive\n",
        "LABELS_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/data/labels\" # Changed to match the correct path in Google Drive\n",
        "# --------------------- Configuration ---------------------\n",
        "\n",
        "# alexnet model (tweakable to increase accuracy)\n",
        "def create_alexnet_model(num_classes, input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    \"\"\"Creates an AlexNet-based model for image classification.\"\"\"\n",
        "\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional Layer 1\n",
        "    x = tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4,\n",
        "                                activation='relu', padding='same')(input_layer)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2)(x)\n",
        "\n",
        "    # Convolutional Layer 2\n",
        "    x = tf.keras.layers.Conv2D(filters=256, kernel_size=5, strides=1,\n",
        "                                activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2)(x)\n",
        "\n",
        "    # Convolutional Layers 3-5\n",
        "    x = tf.keras.layers.Conv2D(filters=384, kernel_size=3, strides=1,\n",
        "                                activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=384, kernel_size=3, strides=1,\n",
        "                                activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1,\n",
        "                                activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2)(x)\n",
        "\n",
        "    # Flatten and Fully Connected Layers\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=outputs)\n",
        "    return model\n",
        "# ADD REGULARIZATION TO AVOID OVERFIT\n",
        "# good\n",
        "def load_data(data_dir, allowed_extensions=('.jpg', '.jpeg', '.png')):\n",
        "    \"\"\"Loads images and their corresponding labels.\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    for label in os.listdir(data_dir):\n",
        "        if label.startswith('.'):\n",
        "            continue\n",
        "        label_dir = os.path.join(data_dir, label)\n",
        "        if os.path.isdir(label_dir):\n",
        "            for img_name in os.listdir(label_dir):\n",
        "                if img_name.startswith('.'):\n",
        "                    continue\n",
        "                if not img_name.lower().endswith(allowed_extensions):\n",
        "                    print(f\"Skipping unsupported file: {img_name}\")\n",
        "                    continue\n",
        "                try:\n",
        "                    img_path = os.path.join(label_dir, img_name)\n",
        "                    img = Image.open(img_path)\n",
        "                    img.verify()\n",
        "                    img = load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
        "                    img_array = img_to_array(img)\n",
        "                    images.append(img_array)\n",
        "                    labels.append(label)\n",
        "                except (IOError, UnidentifiedImageError) as e:\n",
        "                    print(f\"Error loading image {img_name}: {e}\")\n",
        "                    continue\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = load_data(IMAGES_DIR)\n",
        "print(f\"Number of images loaded: {len(X)}\")\n",
        "X = X / 255.0\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "num_classes = len(le.classes_)\n",
        "print(f\"Number of Classes : {num_classes}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to categorical\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Create and compile model\n",
        "model = create_alexnet_model(num_classes)\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    fill_mode=\"nearest\",\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=os.path.join(\n",
        "                MODEL_DIR, \"asl_model_{epoch:02d}_{val_accuracy:.2f}.keras\"\n",
        "            ),\n",
        "            monitor=\"val_accuracy\",\n",
        "            save_best_only=True,\n",
        "            mode=\"max\",\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Save the entire model\n",
        "model.save(os.path.join(MODEL_DIR, \"asl_model.h5\"))\n",
        "\n",
        "# Save the label encoder\n",
        "with open(os.path.join(LABELS_DIR, 'label_encoder.pkl'), 'wb') as f:\n",
        "  pickle.dump(le, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "nsLuHICXluZW",
        "outputId": "3332597a-9b48-4326-e8a7-e0297cee57ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0ec5540e39eb>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/content/drive/My Drive/ASL_to_Text_Project/data\"\u001b[0m \u001b[0;31m# Changed to match the correct path in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mIMAGES_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Path to the 'images' folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mMODEL_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/content/drive/My Drive/ASL_to_Text_Project/models\"\u001b[0m \u001b[0;31m# Changed to match the correct path in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mLABELS_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/content/drive/My Drive/ASL_to_Text_Project/data/labels\"\u001b[0m \u001b[0;31m# Changed to match the correct path in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# past code with alex net"
      ],
      "metadata": {
        "id": "EgUjnXdWlvYO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trEyuxtYElJI"
      },
      "outputs": [],
      "source": [
        "# IMG_SIZE = 227\n",
        "# BATCH_SIZE = 32\n",
        "# EPOCHS = 100\n",
        "# DATA_DIR = \"/content/drive/MyDrive/ASL_to_Text_Project/data\"\n",
        "# MODEL_DIR = \"/content/drive/MyDrive/ASL_to_Text_Project/models\"\n",
        "# LABELS_DIR = \"/content/drive/MyDrive/ASL_to_Text_Project/data/labels\"\n",
        "\n",
        "\n",
        "# # Configuration\n",
        "\n",
        "# def create_improved_model(num_classes):\n",
        "#     model = Sequential([\n",
        "#         Conv2D(64, 11, strides=4, padding=\"same\", activation=\"relu\", input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "#         BatchNormalization(),\n",
        "#         MaxPooling2D(3, strides=2),\n",
        "#         Conv2D(192, 5, padding=\"same\", activation=\"relu\"),\n",
        "#         BatchNormalization(),\n",
        "#         MaxPooling2D(3, strides=2),\n",
        "#         Conv2D(384, 3, padding=\"same\", activation=\"relu\"),\n",
        "#         BatchNormalization(),\n",
        "#         Conv2D(256, 3, padding=\"same\", activation=\"relu\"),\n",
        "#         BatchNormalization(),\n",
        "#         Conv2D(256, 3, padding=\"same\", activation=\"relu\"),\n",
        "#         BatchNormalization(),\n",
        "#         MaxPooling2D(3, strides=2),\n",
        "#         Flatten(),\n",
        "#         Dense(4096, activation=\"relu\"),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(4096, activation=\"relu\"),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(num_classes, activation=\"softmax\")\n",
        "#     ])\n",
        "#     return model\n",
        "\n",
        "# def load_balanced_data(data_dir, img_size=IMG_SIZE, max_samples_per_class=1000):\n",
        "#     images = []\n",
        "#     labels = []\n",
        "#     class_counts = {}\n",
        "#     images_dir = os.path.join(data_dir, \"images\")\n",
        "\n",
        "#     for label in os.listdir(images_dir):\n",
        "#         label_dir = os.path.join(images_dir, label)\n",
        "#         if os.path.isdir(label_dir):\n",
        "#             class_counts[label] = 0\n",
        "#             for img_name in os.listdir(label_dir):\n",
        "#                 if class_counts[label] >= max_samples_per_class:\n",
        "#                     break\n",
        "#                 img_path = os.path.join(label_dir, img_name)\n",
        "#                 if os.path.isfile(img_path):\n",
        "#                     try:\n",
        "#                         img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_size, img_size))\n",
        "#                         img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "#                         images.append(img_array)\n",
        "#                         labels.append(label)\n",
        "#                         class_counts[label] += 1\n",
        "#                     except Exception as e:\n",
        "#                         print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "#     print(\"Class distribution:\")\n",
        "#     for label, count in class_counts.items():\n",
        "#         print(f\"{label}: {count}\")\n",
        "\n",
        "#     return np.array(images), np.array(labels)\n",
        "\n",
        "# def main():\n",
        "#     X, y = load_balanced_data(DATA_DIR)\n",
        "\n",
        "#     if len(X) == 0:\n",
        "#         print(\"No images were loaded. Please check the data directory structure.\")\n",
        "#         return\n",
        "\n",
        "#     print(f\"Loaded {len(X)} images with shape {X.shape}\")\n",
        "#     print(f\"Unique labels: {np.unique(y)}\")\n",
        "\n",
        "#     le = LabelEncoder()\n",
        "#     y_encoded = le.fit_transform(y)\n",
        "#     num_classes = len(le.classes_)\n",
        "\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(\n",
        "#         X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "#     )\n",
        "\n",
        "#     X_train = X_train / 255.0\n",
        "#     X_test = X_test / 255.0\n",
        "\n",
        "#     y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
        "#     y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "#     train_datagen = ImageDataGenerator(\n",
        "#         rotation_range=20,\n",
        "#         width_shift_range=0.2,\n",
        "#         height_shift_range=0.2,\n",
        "#         horizontal_flip=True,\n",
        "#         zoom_range=0.2,\n",
        "#         shear_range=0.2,\n",
        "#         fill_mode='nearest'\n",
        "#     )\n",
        "\n",
        "#     train_generator = train_datagen.flow(\n",
        "#         X_train, y_train, batch_size=BATCH_SIZE, shuffle=True\n",
        "#     )\n",
        "\n",
        "#     model = create_improved_model(num_classes)\n",
        "#     model.compile(\n",
        "#         optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "#         loss=\"categorical_crossentropy\",\n",
        "#         metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "#     )\n",
        "\n",
        "#     early_stopping = EarlyStopping(\n",
        "#         monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
        "#     )\n",
        "#     lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=7)\n",
        "\n",
        "#     history = model.fit(\n",
        "#         train_generator,\n",
        "#         steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "#         epochs=EPOCHS,\n",
        "#         validation_data=(X_test, y_test),\n",
        "#         callbacks=[early_stopping, lr_scheduler]\n",
        "#     )\n",
        "\n",
        "#     test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=2)\n",
        "#     print(f\"\\nTest accuracy: {test_acc}\")\n",
        "#     print(f\"Test precision: {test_precision}\")\n",
        "#     print(f\"Test recall: {test_recall}\")\n",
        "\n",
        "#     os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "#     model_path = os.path.join(MODEL_DIR, \"improved_balanced_asl_model.h5\")\n",
        "#     model.save(model_path)\n",
        "#     print(f\"Model saved successfully: {model_path}\")\n",
        "\n",
        "#     os.makedirs(LABELS_DIR, exist_ok=True)\n",
        "#     label_encoder_path = os.path.join(LABELS_DIR, \"label_encoder.pkl\")\n",
        "#     with open(label_encoder_path, \"wb\") as f:\n",
        "#         pickle.dump(le, f)\n",
        "#     print(f\"Label Encoder saved successfully: {label_encoder_path}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOKpNOQ5oZo1"
      },
      "source": [
        "# training script"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# from tensorflow.keras.applications import MobileNetV2\n",
        "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "# from tensorflow.keras.models import Model, model_from_json\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from PIL import Image, UnidentifiedImageError\n",
        "# import pickle\n",
        "\n",
        "# # Configuration\n",
        "# IMG_SIZE = 227\n",
        "# BATCH_SIZE = 32\n",
        "# EPOCHS = 100  # You can adjust the number of epochs\n",
        "# DATA_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/data\"\n",
        "# IMAGES_DIR = os.path.join(DATA_DIR, 'images') # Path to images folder\n",
        "# MODEL_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/models\"\n",
        "# LABELS_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/data/labels\"\n",
        "\n",
        "# def create_model(num_classes, input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "#     base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "#     x = base_model.output\n",
        "#     x = GlobalAveragePooling2D()(x)\n",
        "#     x = Dense(1024, activation='relu')(x)\n",
        "#     x = Dropout(0.5)(x)\n",
        "#     x = Dense(512, activation='relu')(x)\n",
        "#     x = Dropout(0.5)(x)\n",
        "#     outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "#     model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "#     # Freeze the base model layers\n",
        "#     for layer in base_model.layers:\n",
        "#         layer.trainable = False\n",
        "\n",
        "#     return model\n",
        "\n",
        "# def load_data(data_dir, allowed_extensions=('.jpg', '.jpeg', '.png')):\n",
        "#     images = []\n",
        "#     labels = []\n",
        "#     for label in os.listdir(data_dir):\n",
        "#         if label.startswith('.'):  # Skip hidden files/directories\n",
        "#             continue\n",
        "#         label_dir = os.path.join(data_dir, label)\n",
        "#         if os.path.isdir(label_dir):\n",
        "#             for img_name in os.listdir(label_dir):\n",
        "#                 if img_name.startswith('.'):  # Skip hidden files within label directory\n",
        "#                     continue\n",
        "#                 if not img_name.lower().endswith(allowed_extensions):\n",
        "#                     print(f\"Skipping unsupported file: {img_name}\")\n",
        "#                     continue\n",
        "#                 try:\n",
        "#                     img_path = os.path.join(label_dir, img_name)\n",
        "#                     img = Image.open(img_path)\n",
        "#                     img.verify()\n",
        "#                     img = load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
        "#                     img_array = img_to_array(img)\n",
        "#                     images.append(img_array)\n",
        "#                     labels.append(label)\n",
        "#                 except (IOError, UnidentifiedImageError) as e:\n",
        "#                     print(f\"Error loading image {img_name}: {e}\")\n",
        "#                     continue\n",
        "#     return np.array(images), np.array(labels)\n",
        "\n",
        "# # --- Main Execution ---\n",
        "\n",
        "# # Mount Google Drive (if using Colab)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Load and preprocess data\n",
        "# X, y = load_data(IMAGES_DIR) # Load from the correct directory\n",
        "# print(f\"Number of images loaded: {len(X)}\")\n",
        "# X = X / 255.0  # Normalize pixel values\n",
        "\n",
        "# # Encode labels\n",
        "# le = LabelEncoder()\n",
        "# y_encoded = le.fit_transform(y)\n",
        "# num_classes = len(le.classes_)\n",
        "\n",
        "# # Split data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X, y_encoded, test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "# # Convert to categorical\n",
        "# y_train = to_categorical(y_train, num_classes)\n",
        "# y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# # Create and compile model\n",
        "# model = create_model(num_classes)\n",
        "# model.compile(\n",
        "#     optimizer=Adam(learning_rate=0.0001),\n",
        "#     loss=\"categorical_crossentropy\",\n",
        "#     metrics=[\"accuracy\"],\n",
        "# )\n",
        "\n",
        "# # Data augmentation\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=20,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     horizontal_flip=True,\n",
        "#     zoom_range=0.2,\n",
        "#     shear_range=0.2,\n",
        "#     fill_mode=\"nearest\",\n",
        "# )\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(\n",
        "#     datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "#     steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "#     epochs=EPOCHS,\n",
        "#     validation_data=(X_test, y_test),\n",
        "#     callbacks=[\n",
        "#         tf.keras.callbacks.ModelCheckpoint(\n",
        "#             filepath=os.path.join(\n",
        "#                 MODEL_DIR, \"asl_model_{epoch:02d}_{val_accuracy:.2f}.keras\"\n",
        "#             ),\n",
        "#             monitor=\"val_accuracy\",\n",
        "#             save_best_only=True,\n",
        "#             mode=\"max\",\n",
        "#         )\n",
        "#     ],\n",
        "# )\n",
        "\n",
        "# # Save the entire model\n",
        "# model.save(os.path.join(MODEL_DIR, \"asl_model.h5\"))\n",
        "\n",
        "# # Save the label encoder\n",
        "# with open(os.path.join(LABELS_DIR, 'label_encoder.pkl'), 'wb') as f:\n",
        "#   pickle.dump(le, f)"
      ],
      "metadata": {
        "id": "5h80JxbK9LQi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}